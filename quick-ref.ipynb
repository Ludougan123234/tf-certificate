{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUSVZj8QAMFW"
      },
      "outputs": [],
      "source": [
        "# TODO: Add how to calculate layer parameter\n",
        "# Todo: Add\n",
        "# TODO: Add quick train test split script\n",
        "# TODO: Add quick sequential, functional, sub-classing model building ref\n",
        "# TODO: Add callback ref"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u_mqemSAMFZ"
      },
      "source": [
        "# List of loss functions\n",
        "|  Function  | Formula  | Notes  |\n",
        "|:-------|:-------|:------|\n",
        "| MSE | | |\n",
        "| MAE | | |\n",
        "| Huber | | |\n",
        "| Cross Entropy| | |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1akqyatAMFa"
      },
      "source": [
        "# List of activation functions\n",
        "\n",
        "[Keras API - Full list of activation functions](https://keras.io/api/layers/activations/)\n",
        "|   Function  |   Formula    |   Notes |\n",
        "|:-------|:-------|:------|\n",
        "| Logistic (sigmoid) | $\\sigma(z) = \\frac{1}{1 + exp(-z)}$ | |\n",
        "| Hyperbolic tangent | $\\text{tanh}(z) = 2\\sigma(2z) - 1$ | S-shaped, differentiable. Output ranges (-1, 1) and centered around 0 at beginning of training, giving it faster convergence speed  |\n",
        "| Rectified Linear Unit (ReLU) | $\\text{ReLU}(z) = \\text{max}(0,z)$ |  Continuous, but not differentiable at z = 0, and derivative is 0 for z < 0. Helps computational speed and vanishing gradient during GD (specifically vanishing gradient) |\n",
        "| Softplus | $\\text{softplus}(z) = \\text{log}(1 + \\text{exp}(z))$ | |\n",
        "| Softmax |  |  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5tjGAEXAMFb"
      },
      "source": [
        "# How to calculate layer parameter #:\n",
        "\n",
        "- Dense layer: number of neurons in prev layer * number of neurons in this layer + number of neurons in this layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GhugtYRAMFc"
      },
      "source": [
        "# Choosing the right activation function\n",
        "\n",
        "In general, SELU > ELU > leaky ReLU > ReLU > tanh > logistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhZ5hzuWAMFc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8kguI28AMFc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}