{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: List of loss functions\n",
    "\n",
    "# TODO: List of activation functions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of loss functions \n",
    "|  Function  | Formula  | Notes  | \n",
    "|:-------|:-------|:------|\n",
    "| MSE | | |\n",
    "| MAE | | |\n",
    "| Huber | | |\n",
    "| Cross Entropy| | |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of activation functions \n",
    "\n",
    "[Keras API - Full list of activation functions](https://keras.io/api/layers/activations/)\n",
    "|   Function  |   Formula    |   Notes | \n",
    "|:-------|:-------|:------|\n",
    "| Logistic (sigmoid) | $\\sigma(z) = \\frac{1}{1 + exp(-z)}$ | |\n",
    "| Hyperbolic tangent | $\\text{tanh}(z) = 2\\sigma(2z) - 1$ | S-shaped, differentiable. Output ranges (-1, 1) and centered around 0 at beginning of training, giving it faster convergence speed  |\n",
    "| Rectified Linear Unit (ReLU) | $\\text{ReLU}(z) = \\text{max}(0,z)$ |  Continuous, but not differentiable at z = 0, and derivative is 0 for z < 0. Helps computational speed and vanishing gradient during GD (specifically vanishing gradient) | \n",
    "| Softplus | $\\text{softplus}(z) = \\text{log}(1 + \\text{exp}(z))$ | |\n",
    "|  |  |  | "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
